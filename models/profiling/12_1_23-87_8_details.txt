[02/18/2024-18:51:50] [I] === Trace details ===
[02/18/2024-18:51:50] [I] Trace averages of 10 runs:
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 24.267 ms - Host latency: 24.6822 ms (enqueue 3.41209 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.3336 ms - Host latency: 21.7201 ms (enqueue 3.0064 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.259 ms - Host latency: 21.6465 ms (enqueue 2.97669 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2469 ms - Host latency: 21.641 ms (enqueue 2.9799 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2647 ms - Host latency: 21.6613 ms (enqueue 2.98953 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2685 ms - Host latency: 21.6632 ms (enqueue 2.99169 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2731 ms - Host latency: 21.6683 ms (enqueue 2.96375 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2638 ms - Host latency: 21.6566 ms (enqueue 2.96028 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2606 ms - Host latency: 21.6552 ms (enqueue 2.96113 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2667 ms - Host latency: 21.6544 ms (enqueue 2.95796 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2615 ms - Host latency: 21.6438 ms (enqueue 2.93845 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2788 ms - Host latency: 21.6644 ms (enqueue 2.97229 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2727 ms - Host latency: 21.6572 ms (enqueue 2.94038 ms)
[02/18/2024-18:51:50] [I] Average on 10 runs - GPU latency: 21.2901 ms - Host latency: 21.6744 ms (enqueue 2.96482 ms)
[02/18/2024-18:51:50] [I] 
[02/18/2024-18:51:50] [I] === Performance summary ===
[02/18/2024-18:51:50] [I] Throughput: 46.2255 qps
[02/18/2024-18:51:50] [I] Latency: min = 21.5168 ms, max = 29.4799 ms, mean = 21.8742 ms, median = 21.6716 ms, percentile(90%) = 21.7571 ms, percentile(95%) = 21.8881 ms, percentile(99%) = 28.7915 ms
[02/18/2024-18:51:50] [I] Enqueue Time: min = 2.83447 ms, max = 3.89178 ms, mean = 3.00188 ms, median = 2.96869 ms, percentile(90%) = 3.07024 ms, percentile(95%) = 3.19641 ms, percentile(99%) = 3.75073 ms
[02/18/2024-18:51:50] [I] H2D Latency: min = 0.333984 ms, max = 0.402817 ms, mean = 0.347903 ms, median = 0.343628 ms, percentile(90%) = 0.35791 ms, percentile(95%) = 0.372131 ms, percentile(99%) = 0.39389 ms
[02/18/2024-18:51:50] [I] GPU Compute Time: min = 21.134 ms, max = 29.0406 ms, mean = 21.4829 ms, median = 21.2817 ms, percentile(90%) = 21.3694 ms, percentile(95%) = 21.5014 ms, percentile(99%) = 28.3611 ms
[02/18/2024-18:51:50] [I] D2H Latency: min = 0.0253906 ms, max = 0.0466309 ms, mean = 0.0433582 ms, median = 0.0432739 ms, percentile(90%) = 0.0449219 ms, percentile(95%) = 0.0456543 ms, percentile(99%) = 0.0466309 ms
[02/18/2024-18:51:50] [I] Total Host Walltime: 3.0719 s
[02/18/2024-18:51:50] [I] Total GPU Compute Time: 3.05057 s
[02/18/2024-18:51:50] [W] * GPU compute time is unstable, with coefficient of variance = 5.22416%.
[02/18/2024-18:51:50] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.
[02/18/2024-18:51:50] [I] Explanations of the performance metrics are printed in the verbose logs.
[02/18/2024-18:51:50] [V] 
[02/18/2024-18:51:50] [V] === Explanations of the performance metrics ===
[02/18/2024-18:51:50] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.
[02/18/2024-18:51:50] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.
[02/18/2024-18:51:50] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.
[02/18/2024-18:51:50] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.
[02/18/2024-18:51:50] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.
[02/18/2024-18:51:50] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.
[02/18/2024-18:51:50] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.
[02/18/2024-18:51:50] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.
[02/18/2024-18:51:50] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --verbose --onnx=/home/stampede/Documents/robomaster_cv/models/12_1_23-87_8.onnx --saveEngine=/home/stampede/Documents/robomaster_cv/models/profiling/12_1_23-87_8.engine